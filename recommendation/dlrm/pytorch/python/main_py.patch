diff --git a/recommendation/dlrm/pytorch/python/main.py b/recommendation/dlrm/pytorch/python/main.py
index fae9138..eff6fc8 100755
--- a/recommendation/dlrm/pytorch/python/main.py
+++ b/recommendation/dlrm/pytorch/python/main.py
@@ -15,7 +15,8 @@ import os
 import sys
 import threading
 import time
-from multiprocessing import JoinableQueue
+# from multiprocessing import JoinableQueue
+from faster_fifo import Queue as JoinableQueue
 
 import mlperf_loadgen as lg
 import numpy as np
@@ -391,7 +392,8 @@ class QueueRunner(RunnerBase):
     def __init__(self, model, ds, threads, post_proc=None, max_batchsize=128):
         super().__init__(model, ds, threads, post_proc, max_batchsize)
         queue_size_multiplier = 4 #(args.samples_per_query_offline + max_batchsize - 1) // max_batchsize)
-        self.tasks = JoinableQueue(maxsize=threads * queue_size_multiplier)
+        # self.tasks = JoinableQueue(maxsize=threads * queue_size_multiplier)
+        self.tasks = JoinableQueue(threads * queue_size_multiplier)
         self.workers = []
         self.result_dict = {}
 
@@ -404,28 +406,39 @@ class QueueRunner(RunnerBase):
     def handle_tasks(self, tasks_queue):
         """Worker thread."""
         while True:
-            qitem = tasks_queue.get()
-            if qitem is None:
-                # None in the queue indicates the parent want us to exit
-                tasks_queue.task_done()
-                break
-            self.run_one_item(qitem)
-            tasks_queue.task_done()
+            # qitem = tasks_queue.get()
+            # if qitem is None:
+            #      # None in the queue indicates the parent want us to exit
+            #      # tasks_queue.task_done()
+            #      break
+            # self.run_one_item(qitem)
+            # tasks_queue.task_done()
+            qitem = tasks_queue.get_many(max_messages_to_get=4)
+            for q in qitem:
+                if q is None:
+                    return
+                self.run_one_item(q)
 
     def enqueue(self, query_samples):
         idx = [q.index for q in query_samples]
         query_id = [q.id for q in query_samples]
         query_len = len(query_samples)
 
+        tasks_list = []
         if query_len < self.max_batchsize:
             batch_dense_X, batch_lS_o, batch_lS_i, batch_T, idx_offsets = self.ds.get_samples(idx)
-            self.tasks.put(Item(query_id, idx, batch_dense_X, batch_lS_o, batch_lS_i, batch_T, idx_offsets))
+            # self.tasks.put(Item(query_id, idx, batch_dense_X, batch_lS_o, batch_lS_i, batch_T, idx_offsets))
+            tasks_lists.append(Item(query_id, idx, batch_dense_X, batch_lS_o, batch_lS_i, batch_T, idx_offsets))
         else:
             bs = self.max_batchsize
             for i in range(0, query_len, bs):
                 ie = min(i + bs, query_len)
                 batch_dense_X, batch_lS_o, batch_lS_i, batch_T, idx_offsets = self.ds.get_samples(idx[i:ie])
-                self.tasks.put(Item(query_id[i:ie], idx[i:ie], batch_dense_X, batch_lS_o, batch_lS_i, batch_T, idx_offsets))
+                # self.tasks.put(Item(query_id[i:ie], idx[i:ie], batch_dense_X, batch_lS_o, batch_lS_i, batch_T, idx_offsets))
+				tasks_list.append(Item(query_id[i:ie], idx[i:ie], batch_dense_X, batch_lS_o, batch_lS_i, batch_T, idx_offsets))
+
+        # Put as many items in the queue
+        self.tasks.put_many(tasks_list)
 
     def finish(self):
         # exit all threads
